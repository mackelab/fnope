{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from fnope.utils.misc import get_output_dir, read_pickle\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = get_output_dir()\n",
    "\n",
    "\n",
    "experiment_folder = out_dir / \"linear_gaussian_experiment\"\n",
    "methods = [\n",
    "    \"FNOPE_equispaced_False\",\n",
    "    \"FNOPE_always_equispaced_True\",\n",
    "    \"spectral_NPE\",\n",
    "    \"raw_FMPE\",\n",
    "    \"spectral_FMPE\"]\n",
    "nsims = [100, 1_000, 10_000, 100_000]\n",
    "\n",
    "import pandas as pd\n",
    "lg_df = pd.DataFrame()\n",
    "for method in methods:\n",
    "    for nsim in nsims:\n",
    "        for run in range(1,4):\n",
    "            print(method,nsim,run)\n",
    "\n",
    "            swd_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"swd_results.pkl\")\n",
    "            swds = swd_res[\"swd\"]\n",
    "            tarp_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"tarp_results.pkl\")\n",
    "            tarp_absolute_atcs = tarp_res[\"absolute_atcs\"]\n",
    "            sbc_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"sbc_results.pkl\")\n",
    "            sbc_absolute_atcs = sbc_res[\"absolute_atcs\"]\n",
    "            predictive_mse_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"predictive_check_results.pkl\")\n",
    "            predictive_mse = predictive_mse_res[\"mses\"]\n",
    "            random_seed_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"random_seed.csv\"\n",
    "            random_seed = int(np.loadtxt(random_seed_path, delimiter=\",\"))\n",
    "\n",
    "            training_time_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"training_time.csv\"\n",
    "            training_time_csv = np.loadtxt(training_time_path, delimiter=\",\")\n",
    "            num_epochs = training_time_csv[0]\n",
    "            training_time = training_time_csv[1]  # in seconds\n",
    "            time_per_epoch = training_time / num_epochs  # in seconds\n",
    "\n",
    "            sampling_time_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"sampling_times.csv\"\n",
    "            sampling_times_df = pd.read_csv(sampling_time_path)\n",
    "            print(sampling_times_df.head())\n",
    "            sampling_times_df[\"time_per_sample\"] = sampling_times_df[\"time_ns\"]/sampling_times_df[\"num_samples\"]\n",
    "            sampling_times = sampling_times_df[\"time_per_sample\"].mean()\n",
    "            sampling_times_ms = sampling_times * 1e-6  # Convert from ns to milliseconds\n",
    "\n",
    "            nn_param_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"nn_num_params.csv\"\n",
    "            nn_num_params = int(np.loadtxt(nn_param_path, delimiter=\",\"))\n",
    "\n",
    "            # print(random_seeds[method][nsim][run-1])\n",
    "            # print(f\"swd: {swds.shape}\")\n",
    "            # print(f\"tarp: {tarp_absolute_atcs}\")\n",
    "            # print(f\"sbc: {sbc_absolute_atcs.shape}\")\n",
    "            # print(f\"predictive_mse: {predictive_mse.flatten().shape}\")\n",
    "            # print(f\"random_seed: {random_seed}\")\n",
    "            # print(f\"training_time: {training_time}\")\n",
    "            # print(f\"sampling_time: {sampling_times_ms}\")\n",
    "            # print(f\"nn_num_params: {nn_num_params}\")\n",
    "\n",
    "\n",
    "            lg_df = pd.concat([lg_df, pd.DataFrame({\n",
    "                \"method\": method,\n",
    "                \"nsim\": nsim,\n",
    "                \"random_seed\": random_seed,\n",
    "                \"swds\": [swds.cpu().numpy()],\n",
    "                \"tarps\": tarp_absolute_atcs,\n",
    "                \"sbcs\": [sbc_absolute_atcs.cpu().numpy()],\n",
    "                \"predictive_mses\": [predictive_mse.cpu().numpy()],\n",
    "                \"training_epochs\": num_epochs,\n",
    "                \"training_time_total\": training_time,\n",
    "                \"training_time_per_epoch\": time_per_epoch,\n",
    "                \"sampling_time_per_sample\": sampling_times_ms,\n",
    "                \"nn_num_params\": nn_num_params,\n",
    "            },)], ignore_index=True)\n",
    "\n",
    "lg_df.to_csv(experiment_folder / \"summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = Path(out_dir/\"sir_experiment\" / \"FNOPE\")\n",
    "methods = [\"fno\",\"simformer\"]\n",
    "nsims = [1_000, 10_000, 100_000]\n",
    "sir_df = pd.DataFrame()\n",
    "for method in methods:\n",
    "    for nsim in nsims:\n",
    "        for run in range(1,4):\n",
    "\n",
    "            tarp_res = read_pickle(experiment_folder / f\"num_sim_{nsim}_run_{run}\" / f\"{method}_tarp_results.pkl\")\n",
    "            tarp_absolute_atcs = tarp_res[\"absolute_atcs\"]\n",
    "            sbc_res = read_pickle(experiment_folder / f\"num_sim_{nsim}_run_{run}\"  / f\"{method}_sbc_results.pkl\")\n",
    "            sbc_absolute_atcs = sbc_res[\"absolute_atcs\"]\n",
    "            predictive_mse_res = read_pickle(experiment_folder / f\"num_sim_{nsim}_run_{run}\" / f\"{method}_predictive_check_results.pkl\")\n",
    "            predictive_mse = predictive_mse_res[\"mses\"]\n",
    "            print(\"predictive mse shape: \", predictive_mse.shape)\n",
    "            random_seed_path = experiment_folder / f\"num_sim_{nsim}_run_{run}\" / f\"random_seed.csv\"\n",
    "            random_seed = int(np.loadtxt(random_seed_path, delimiter=\",\"))\n",
    "\n",
    "\n",
    "            training_time_path = experiment_folder /  f\"num_sim_{nsim}_run_{run}\" / \"training_time.csv\"\n",
    "            training_time_csv = np.loadtxt(training_time_path, delimiter=\",\")\n",
    "            num_epochs = training_time_csv[0]\n",
    "            training_time = training_time_csv[1]  # in seconds\n",
    "            time_per_epoch = training_time / num_epochs  # in seconds\n",
    "\n",
    "            sampling_time_path = experiment_folder /  f\"num_sim_{nsim}_run_{run}\" / \"sampling_times.csv\"\n",
    "            sampling_times_df = pd.read_csv(sampling_time_path)\n",
    "            print(sampling_times_df.head())\n",
    "            sampling_times_df[\"time_per_sample\"] = sampling_times_df[\"time_ns\"]/sampling_times_df[\"num_samples\"]\n",
    "            sampling_times = sampling_times_df[\"time_per_sample\"].mean()\n",
    "            sampling_times_ms = sampling_times * 1e-6  # Convert from ns to milliseconds\n",
    "\n",
    "            nn_param_path = experiment_folder / f\"num_sim_{nsim}_run_{run}\" / \"nn_num_params.csv\"\n",
    "            nn_num_params = int(np.loadtxt(nn_param_path, delimiter=\",\"))\n",
    "            # print(random_seeds[method][nsim][run-1])\n",
    "            # print(f\"method: {method}, num_sim: {nsim}, run: {run}\")\n",
    "            # print(f\"tarp: {tarp_absolute_atcs}\")\n",
    "            # print(f\"sbc: {sbc_absolute_atcs.shape}\")\n",
    "            # print(f\"predictive_mse: {predictive_mse.flatten().mean()}\")\n",
    "            # print(f\"random_seed: {random_seed}\")\n",
    "            # print(f\"training_time: {training_time}\")\n",
    "            # print(f\"sampling_time: {sampling_times_ms}\")\n",
    "            # print(f\"nn_num_params: {nn_num_params}\")\n",
    "\n",
    "            sir_df = pd.concat([sir_df, pd.DataFrame({\n",
    "                \"method\": method,\n",
    "                \"nsim\": nsim,\n",
    "                \"random_seed\": random_seed,\n",
    "                \"tarps\": tarp_absolute_atcs,\n",
    "                \"sbcs\": [sbc_absolute_atcs.cpu().numpy()],\n",
    "                \"predictive_mses\": [predictive_mse.cpu().numpy()],\n",
    "                \"training_epochs\": num_epochs,\n",
    "                \"training_time_total\": training_time,\n",
    "                \"training_time_per_epoch\": time_per_epoch,\n",
    "                \"sampling_time_per_sample\": sampling_times_ms,\n",
    "                \"nn_num_params\": nn_num_params,\n",
    "            },)], ignore_index=True)\n",
    "\n",
    "\n",
    "sir_df.to_pickle(experiment_folder / \"summary.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = out_dir / \"darcy_experiment\"\n",
    "methods = [\n",
    "    \"FNOPE_always_equispaced_False\",\n",
    "    \"FNOPE_always_equispaced_True\",\n",
    "    \"spectral_NPE\",\n",
    "    \"raw_FMPE\",\n",
    "    \"spectral_FMPE\"]\n",
    "\n",
    "nsims = [100, 1_000, 3_000, 10_000]\n",
    "param_dim = 129  # for the darcy experiment, the parameter dimension is 129\n",
    "\n",
    "\n",
    "darcy_df = pd.DataFrame()\n",
    "for method in methods:\n",
    "    for nsim in nsims:\n",
    "        for run in range(1,4):\n",
    "            print(method,nsim,run)\n",
    "\n",
    "\n",
    "            tarp_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"tarp_results.pkl\")\n",
    "            tarp_absolute_atcs = tarp_res[\"absolute_atcs\"]\n",
    "            sbc_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"sbc_results.pkl\")\n",
    "            sbc_absolute_atcs = sbc_res[\"absolute_atcs\"]\n",
    "            predictive_mse_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"predictive_check_results.pkl\")\n",
    "            predictive_mse = predictive_mse_res[\"mses\"]\n",
    "            prior_log_prob_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"prior_log_probs.pkl\")\n",
    "            prior_log_probs = (prior_log_prob_res[\"prior_log_probs\"].view(-1)/(param_dim**2)).cpu().numpy()\n",
    "            if method in [\"FNOPE_always_equispaced_True\",\"FNOPE_always_equispaced_False\",\"raw_FMPE\"]:\n",
    "                posterior_log_prob_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"posterior_log_probs.pkl\")\n",
    "                posterior_log_probs = (posterior_log_prob_res[\"posterior_log_probs\"]/(param_dim**2)).cpu().numpy()\n",
    "            else:\n",
    "                posterior_log_probs = np.nan\n",
    "            random_seed_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"random_seed.csv\"\n",
    "            random_seed = int(np.loadtxt(random_seed_path, delimiter=\",\"))\n",
    "            # print(random_seeds[method][nsim][run-1])\n",
    "\n",
    "            training_time_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"training_time.csv\"\n",
    "            training_time_csv = np.loadtxt(training_time_path, delimiter=\",\")\n",
    "            num_epochs = training_time_csv[0]\n",
    "            training_time = training_time_csv[1]  # in seconds\n",
    "            time_per_epoch = training_time / num_epochs  # in seconds\n",
    "\n",
    "            sampling_time_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"sampling_times.csv\"\n",
    "            sampling_times_df = pd.read_csv(sampling_time_path)\n",
    "            print(sampling_times_df.head())\n",
    "            sampling_times_df[\"time_per_sample\"] = sampling_times_df[\"time_ns\"]/sampling_times_df[\"num_samples\"]\n",
    "            sampling_times = sampling_times_df[\"time_per_sample\"].mean()\n",
    "            sampling_times_ms = sampling_times * 1e-6  # Convert from ns to milliseconds\n",
    "\n",
    "            nn_param_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"nn_num_params.csv\"\n",
    "            nn_num_params = int(np.loadtxt(nn_param_path, delimiter=\",\"))\n",
    "\n",
    "            # print(f\"tarp: {tarp_absolute_atcs}\")\n",
    "            # print(f\"sbc: {sbc_absolute_atcs.shape}\")\n",
    "            # print(f\"predictive_mse: {predictive_mse.flatten().shape}\")\n",
    "            # print(f\"random_seed: {random_seed}\")\n",
    "            # print(f\"prior_log_probs: {prior_log_probs.shape}\")\n",
    "            # print(f\"posterior_log_probs: {posterior_log_probs}\")\n",
    "            # print(f\"training_time: {training_time}\")\n",
    "            # print(f\"sampling_time: {sampling_times_ms}\")\n",
    "            # print(f\"nn_num_params: {nn_num_params}\")\n",
    "\n",
    "\n",
    "            darcy_df = pd.concat([darcy_df, pd.DataFrame({\n",
    "                \"method\": method,\n",
    "                \"nsim\": nsim,\n",
    "                \"random_seed\": random_seed,\n",
    "                \"tarps\": tarp_absolute_atcs,\n",
    "                \"sbcs\": [sbc_absolute_atcs.cpu().numpy()],\n",
    "                \"predictive_mses\": [predictive_mse.cpu().numpy()],\n",
    "                \"prior_log_probs\": [prior_log_probs],\n",
    "                \"posterior_log_probs\": [posterior_log_probs],\n",
    "                \"training_epochs\": num_epochs,\n",
    "                \"training_time_total\": training_time,\n",
    "                \"training_time_per_epoch\": time_per_epoch,\n",
    "                \"sampling_time_per_sample\": sampling_times_ms,\n",
    "                \"nn_num_params\": nn_num_params,\n",
    "            },)], ignore_index=True)\n",
    "\n",
    "darcy_df.to_csv(experiment_folder / \"summary.csv\", index=False)\n",
    "darcy_df.to_pickle(experiment_folder / \"summary.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "darcy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ice Results go here\n",
    "\n",
    "experiment_folder = out_dir / \"ice_experiment\"\n",
    "methods = [\n",
    "    \"FNOPE_always_equispaced_False\",\n",
    "    \"spectral_NPE\",\n",
    "    \"raw_FMPE\",\n",
    "    \"spectral_FMPE\",\n",
    "    \"raw_NPE\"]\n",
    "\n",
    "nsims = [100, 1_000, 10_000, 100_000]\n",
    "x_size = 441\n",
    "\n",
    "\n",
    "ice_df = pd.DataFrame()\n",
    "for method in methods:\n",
    "    for nsim in nsims:\n",
    "        for run in range(1,4):\n",
    "            print(method,nsim,run)\n",
    "\n",
    "            tarp_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"tarp_results.pkl\")\n",
    "            tarp_absolute_atcs = tarp_res[\"absolute_atcs\"]\n",
    "            sbc_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"sbc_results.pkl\")\n",
    "            sbc_absolute_atcs = sbc_res[\"absolute_atcs\"]\n",
    "            predictive_mse_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"predictive_check_results.pkl\")\n",
    "            predictive_mse = predictive_mse_res[\"mses\"]/x_size\n",
    "            predictive_mse_real_data_res = read_pickle(experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"real_layers_predictive_check_results.pkl\")\n",
    "            predictive_mse_real_data = predictive_mse_real_data_res[\"mses\"]\n",
    "            random_seed_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / f\"random_seed.csv\"\n",
    "            random_seed = int(np.loadtxt(random_seed_path, delimiter=\",\"))\n",
    "            # print(random_seeds[method][nsim][run-1])\n",
    "\n",
    "            training_time_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"training_time.csv\"\n",
    "            training_time_csv = np.loadtxt(training_time_path, delimiter=\",\")\n",
    "            num_epochs = training_time_csv[0]\n",
    "            training_time = training_time_csv[1]  # in seconds\n",
    "            time_per_epoch = training_time / num_epochs  # in seconds\n",
    "\n",
    "            sampling_time_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"sampling_times.csv\"\n",
    "            sampling_times_df = pd.read_csv(sampling_time_path)\n",
    "            print(sampling_times_df.head())\n",
    "            sampling_times_df[\"time_per_sample\"] = sampling_times_df[\"time_ns\"]/sampling_times_df[\"num_samples\"]\n",
    "            sampling_times = sampling_times_df[\"time_per_sample\"].mean()\n",
    "            sampling_times_ms = sampling_times * 1e-6  # Convert from ns to milliseconds\n",
    "\n",
    "            nn_param_path = experiment_folder / method / f\"num_sim_{nsim}_run_{run}\" / \"nn_num_params.csv\"\n",
    "            nn_num_params = int(np.loadtxt(nn_param_path, delimiter=\",\"))\n",
    "\n",
    "\n",
    "            # print(f\"tarp: {tarp_absolute_atcs}\")\n",
    "            # print(f\"sbc: {sbc_absolute_atcs.shape}\")\n",
    "            # print(f\"predictive_mse: {predictive_mse.flatten().shape}\")\n",
    "            # print(f\"random_seed: {random_seed}\")\n",
    "            # print(f\"predictive_mse_real_data: {predictive_mse_real_data.flatten().shape}\")\n",
    "            # print(f\"training_time: {training_time}\")\n",
    "            # print(f\"sampling_time: {sampling_times_ms}\")\n",
    "            # print(f\"nn_num_params: {nn_num_params}\")\n",
    "\n",
    "            ice_df = pd.concat([ice_df, pd.DataFrame({\n",
    "                \"method\": method,\n",
    "                \"nsim\": nsim,\n",
    "                \"random_seed\": random_seed,\n",
    "                \"tarps\": tarp_absolute_atcs,\n",
    "                \"sbcs\": [sbc_absolute_atcs.cpu().numpy()],\n",
    "                \"predictive_mses\": [predictive_mse.cpu().numpy()],\n",
    "                \"predictive_mses_real_data\": [predictive_mse_real_data.cpu().numpy()],\n",
    "                \"training_epochs\": num_epochs,\n",
    "                \"training_time_total\": training_time,\n",
    "                \"training_time_per_epoch\": time_per_epoch,\n",
    "                \"sampling_time_per_sample\": sampling_times_ms,\n",
    "                \"nn_num_params\": nn_num_params\n",
    "            },)], ignore_index=True)\n",
    "\n",
    "ice_df.to_csv(experiment_folder / \"summary.csv\", index=False)\n",
    "ice_df.to_pickle(experiment_folder / \"summary.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fb635",
   "metadata": {},
   "source": [
    "# Combine to generate markdown table of compute times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\"FNOPE_always_equispaced_False\": \"FNOPE\",\n",
    "           \"FNOPE_always_equispaced_True\": \"FNOPE (fix)\",\n",
    "           \"spectral_NPE\": \"Spectral NPE\",\n",
    "           \"raw_FMPE\": \"Raw FMPE\",\n",
    "           \"spectral_FMPE\": \"Spectral FMPE\",\n",
    "           \"raw_NPE\": \"Raw NPE\",\n",
    "        #    \"Simformer\": \"Simformer\"\n",
    "           }\n",
    "\n",
    "task_names = {\"Gaussian_Linear\": lg_df,\n",
    "            #   \"SIRD\": sir_df,\n",
    "              \"Darcy\": darcy_df,\n",
    "              \"ice\": ice_df}\n",
    "\n",
    "which_nsim = 10_000\n",
    "\n",
    "# Separate dictionaries for each metric per task\n",
    "\n",
    "\n",
    "nn_params = {}\n",
    "training_total = {}\n",
    "training_per_epoch = {}\n",
    "sampling_per_sample = {}\n",
    "\n",
    "# Error dictionaries\n",
    "training_total_err = {}\n",
    "training_per_epoch_err = {}\n",
    "sampling_per_sample_err = {}\n",
    "\n",
    "for task,df in task_names.items():\n",
    "    nn_params[task] = {}\n",
    "    training_total[task] = {}\n",
    "    training_per_epoch[task] = {}\n",
    "    sampling_per_sample[task] = {}\n",
    "\n",
    "    # Error dictionaries\n",
    "    training_total_err[task] = {}\n",
    "    training_per_epoch_err[task] = {}\n",
    "    sampling_per_sample_err[task] = {}\n",
    "\n",
    "    for method in methods.keys():\n",
    "        df_view = df[(df[\"method\"] == method) & (df[\"nsim\"] == which_nsim)]\n",
    "        if df_view.empty:\n",
    "            nn_params[task][method] = np.nan\n",
    "            training_total[task][method] = np.nan\n",
    "            training_per_epoch[task][method] = np.nan\n",
    "            sampling_per_sample[task][method] = np.nan\n",
    "            training_total_err[task][method] = np.nan\n",
    "            training_per_epoch_err[task][method] = np.nan\n",
    "            sampling_per_sample_err[task][method] = np.nan\n",
    "        else:\n",
    "            nn_params[task][method] = int(df_view[\"nn_num_params\"].values[0])\n",
    "            training_total[task][method] = df_view[\"training_time_total\"].values.mean()\n",
    "            training_per_epoch[task][method] = df_view[\"training_time_per_epoch\"].values.mean()\n",
    "            sampling_per_sample[task][method] = df_view[\"sampling_time_per_sample\"].values.mean()\n",
    "            training_total_err[task][method] = df_view[\"training_time_total\"].values.std()\n",
    "            training_per_epoch_err[task][method] = df_view[\"training_time_per_epoch\"].values.std()\n",
    "            sampling_per_sample_err[task][method] = df_view[\"sampling_time_per_sample\"].values.std()\n",
    "\n",
    "\n",
    "print(\"training_time_total:\", training_total)\n",
    "print(\"training_time_per_epoch:\", training_per_epoch)\n",
    "print(\"nn_params:\", nn_params)\n",
    "# Build the dataframe\n",
    "table_df = pd.DataFrame({(\"method\", \"\"): methods.values()})\n",
    "\n",
    "# Helper to format ± values\n",
    "def format_with_uncertainty(mean, std):\n",
    "    if pd.isna(mean):\n",
    "        return \"nan\"\n",
    "    if mean > 1000:\n",
    "        return f\"{mean:.2e} ± {std:.2e}\"\n",
    "    else:\n",
    "        return f\"{mean:.2f} ± {std:.2f}\"\n",
    "\n",
    "for task in task_names:\n",
    "    table_df[(task, \"# weights\")] = nn_params[task].values()\n",
    "    table_df[(task, \"training (Tot.). [s]\")] = [\n",
    "        format_with_uncertainty(v, e)\n",
    "        for v, e in zip(training_total[task].values(), training_total_err[task].values())\n",
    "    ]\n",
    "    table_df[(task, \"training (/epoch) [s]\")] = [\n",
    "        format_with_uncertainty(v, e)\n",
    "        for v, e in zip(training_per_epoch[task].values(), training_per_epoch_err[task].values())\n",
    "    ]\n",
    "    table_df[(task, \"sampling time [ms]\")] = [\n",
    "        format_with_uncertainty(v, e)\n",
    "        for v, e in zip(sampling_per_sample[task].values(), sampling_per_sample_err[task].values())\n",
    "    ]\n",
    "\n",
    "# MultiIndex for headers\n",
    "table_df.columns = pd.MultiIndex.from_tuples(table_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe601d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the DataFrame\n",
    "table_df_transposed = table_df.T\n",
    "\n",
    "# If you want to keep the MultiIndex columns, transpose will convert columns to rows and vice versa.\n",
    "# Then generate the markdown:\n",
    "# Print markdown without column labels\n",
    "# print(table_df_transposed.to_markdown(headers=[]))\n",
    "\n",
    "def transpose_to_markdown(\n",
    "    df: pd.DataFrame,\n",
    "    method_column_name=(\"method\", \"\"),\n",
    "    param_metric=\"# weights\"\n",
    ") -> str:\n",
    "    if not isinstance(df.columns, pd.MultiIndex):\n",
    "        raise ValueError(\"Expected MultiIndex columns\")\n",
    "\n",
    "    # Step 1: Set method as index\n",
    "    if method_column_name in df.columns:\n",
    "        df = df.set_index(method_column_name)\n",
    "    else:\n",
    "        raise ValueError(f\"'{method_column_name}' not found in columns\")\n",
    "\n",
    "    # Step 2: Keep only data columns\n",
    "    data = df.loc[:, df.columns.get_level_values(0) != method_column_name[0]]\n",
    "    df_t = data.T  # Transpose\n",
    "\n",
    "    # Step 3: Header\n",
    "    methods = df.index.tolist()\n",
    "    header = ['Task', 'Metric'] + [str(m) for m in methods]\n",
    "    separator = ['---'] * len(header)\n",
    "\n",
    "    lines = [\n",
    "        '| ' + ' | '.join(header) + ' |',\n",
    "        '| ' + ' | '.join(separator) + ' |'\n",
    "    ]\n",
    "\n",
    "    last_task = None\n",
    "    for (task, metric) in df_t.index:\n",
    "        if task != last_task and last_task is not None:\n",
    "            lines.append('| ' + ' | '.join([''] * len(header)) + ' |')\n",
    "\n",
    "        row = [str(task), str(metric)]\n",
    "        for m in methods:\n",
    "            value = df_t.loc[(task, metric), m]\n",
    "            if metric == param_metric:\n",
    "                if pd.isna(value):\n",
    "                    row.append(\"nan\")\n",
    "                else:\n",
    "                    row.append(f\"{int(value):,}\")\n",
    "            else:\n",
    "                row.append(str(value))\n",
    "        lines.append('| ' + ' | '.join(row) + ' |')\n",
    "        last_task = task\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(transpose_to_markdown(table_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d76c4",
   "metadata": {},
   "source": [
    "| Task | Metric | FNOPE | FNOPE (fix) | Spectral NPE | Raw FMPE | Spectral FMPE | Raw NPE |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Gaussian_Linear | # weights | 109,797 | 108,581 | 566,590 | 299,310 | 86,302 | nan |\n",
    "| Gaussian_Linear | training (Tot.). [s] | 187.29 ± 43.20 | 118.53 ± 9.05 | 405.94 ± 35.88 | 308.39 ± 1.56 | 147.87 ± 7.21 | nan |\n",
    "| Gaussian_Linear | training (/epoch) [s] | 0.99 ± 0.09 | 0.87 ± 0.08 | 2.17 ± 0.05 | 0.31 ± 0.00 | 0.24 ± 0.00 | nan |\n",
    "| Gaussian_Linear | sampling time [ms] | 2.16 ± 0.12 | 1.06 ± 0.10 | 0.05 ± 0.00 | 0.20 ± 0.03 | 0.24 ± 0.02 | nan |\n",
    "|  |  |  |  |  |  |  |  |\n",
    "| Darcy | # weights | 11,555,673 | 11,552,521 | 3,537,566 | 9,180,033 | 897,664 | nan |\n",
    "| Darcy | training (Tot.). [s] | 4.33e+03 ± 1.33e+03 | 2.51e+03 ± 4.26e+01 | 1.23e+03 ± 3.41e+02 | 732.19 ± 5.31 | 656.72 ± 1.53 | nan |\n",
    "| Darcy | training (/epoch) [s] | 20.30 ± 0.63 | 26.23 ± 0.12 | 2.82 ± 0.05 | 0.73 ± 0.01 | 0.66 ± 0.00 | nan |\n",
    "| Darcy | sampling time [ms] | 279.55 ± 6.64 | 35.20 ± 3.58 | 0.21 ± 0.01 | 2.70 ± 0.10 | 1.95 ± 0.07 | nan |\n",
    "|  |  |  |  |  |  |  |  |\n",
    "| ice | # weights | 25,317 | nan | 235,608 | 96,210 | 92,340 | 360,933 |\n",
    "| ice | training (Tot.). [s] | 2.84e+03 ± 8.67e+02 | nan | 1.18e+03 ± 1.64e+02 | 2.01e+03 ± 2.93e+01 | 1.90e+03 ± 9.84e+01 | 407.97 ± 13.39 |\n",
    "| ice | training (/epoch) [s] | 9.82 ± 0.12 | nan | 4.21 ± 0.37 | 2.02 ± 0.04 | 1.92 ± 0.07 | 5.64 ± 0.07 |\n",
    "| ice | sampling time [ms] | 22.79 ± 2.21 | nan | 1.50 ± 0.15 | 23.85 ± 0.63 | 16.61 ± 1.30 | 1.53 ± 0.06 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc4805",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fourier_nets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
