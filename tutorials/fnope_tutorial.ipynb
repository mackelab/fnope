{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from fnope.simulators.simulator import SIR\n",
    "from fnope.simulators.gp_priors import get_gaussian_process_prior_1d\n",
    "from fnope.flow_matching.fnope_1D import FNOPE_1D\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3429c7b",
   "metadata": {},
   "source": [
    "## SIR Model\n",
    "\n",
    "The SIR example contains several parameter channels, as well as additional vector-valued parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b943aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior\n",
    "\n",
    "seq_len = 100 #Simulation gridpoints\n",
    "T = 50 #length of simulation\n",
    "\n",
    "func_prior = get_gaussian_process_prior_1d(num_points=seq_len, domain_length = T, mean=0.0, lengthscale = 7.0, sigma=2.5)\n",
    "vec_prior = torch.distributions.Uniform(torch.Tensor([0.0, 0.0]), torch.Tensor([0.5, 0.5])) #gamma,mu\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate prior samples\n",
    "\n",
    "num_sims = 5000\n",
    "likelihood_scale = 0.05 #i.i.d noise\n",
    "\n",
    "\n",
    "#sample evenly spaced simulation grid\n",
    "sim_ts = torch.linspace(0, T, seq_len).to(device)\n",
    "\n",
    "# We could already simulate each prior sample on a non-uniform grid, if our simulator supports this.\n",
    "# sim_ts = torch.rand(seq_len).to(device)\n",
    "# sim_ts = torch.sort(sim_ts)[0]\n",
    "# sim_ts *= T\n",
    "\n",
    "\n",
    "sim_theta_func = func_prior.sample((num_sims,)).to(device)\n",
    "sim_theta_func = torch.sigmoid(sim_theta_func)\n",
    "\n",
    "sim_theta_vec = vec_prior.sample((num_sims,)).to(device)\n",
    "\n",
    "# delta = torch.rand(num_sims)*0.09+0.01\n",
    "\n",
    "sim_x = SIR(sim_theta_func,sim_ts,gamma=sim_theta_vec[:,0],delta=sim_theta_vec[:,1],likelihood_scale=likelihood_scale,device=device)\n",
    "\n",
    "#Also generate ground truth\n",
    "theta_func_o = func_prior.sample(torch.Size([10])).to(device)\n",
    "theta_func_o = torch.sigmoid(theta_func_o)\n",
    "theta_vec_o = vec_prior.sample((10,)).to(device)\n",
    "x_o = SIR(theta_func_o, sim_ts, gamma=theta_vec_o[:,0], delta=theta_vec_o[:,1], likelihood_scale=likelihood_scale, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure timeseries are in shape (Batch, Channel, Time)\n",
    "sim_theta_func  = sim_theta_func.view(-1, 1, seq_len)\n",
    "sim_x = sim_x.view(-1, 3, seq_len)\n",
    "theta_func_o = theta_func_o.view(-1, 1, seq_len)\n",
    "x_o = x_o.view(-1, 3, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dc2c7",
   "metadata": {},
   "source": [
    "## Define FNOPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes_max = 32 #number of modes used by FNO blocks\n",
    "\n",
    "\n",
    "model = FNOPE_1D(\n",
    "    x = sim_theta_func, #note: x in `FNOPE` models is the parameter (theta), `ctx` is the observation\n",
    "    ctx=sim_x,\n",
    "    simulation_grid=sim_ts/T, #We normalize the grid to [0,1] - this means we also need to normalize the grid to [0,1] when evaluating!\n",
    "    x_finite=sim_theta_vec, #vector-valued parameters (\"finite\")\n",
    "    modes= modes_max, \n",
    "    conv_channels = 16,\n",
    "    ctx_embedding_channels=8,\n",
    "    time_embedding_channels=4,\n",
    "    position_embedding_channels=4,\n",
    "    num_layers=5,\n",
    "    base_dist='gp', #This is the type of distribution used as the base distribution for flow matching. We use a Gaussian Process (the lengthscale is set depending on `modes`).\n",
    "    padding = {\"type\":\"none\",\"pad_length\":0}, #when using FFT, padding is helpful to avoid artefacts.\n",
    "    training_point_noise={\n",
    "         \"jitter\": 0.001 #scale of noise added to each position independently\n",
    "         ,\"target_gridsize\": 50 #amount of points left over for each training sample after masking\n",
    "           }, #This becomes unnecessary if we are already passing in simulations on different grids.\n",
    "    always_equispaced=False, # Set to True for FNOPE (fix) - always evaluating on the same grid as the training data\n",
    "    always_match_x_theta=False, #If always_equispaced=True, can also specify whether parameters and observations are defined on the same grid.\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a48339",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "#Set up custom training here, training function also at fnope.flow_matching.training.py::train_fnope\n",
    "dataset = TensorDataset(sim_theta_func, sim_theta_vec, sim_x)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f35003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "num_epochs = 400\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0.0\n",
    "    for theta_func_batch,theta_vec_batch, x_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        #when the model is defined with always_match_x_theta=False and always_equispaced=False, you need to pass the simulation positions for both\n",
    "        #theta and x (simulation_positions and ctx_simulation_positions), EVEN if they're the same. This for BOTH training AND evaluation.\n",
    "        loss = model.loss(theta_func_batch, ctx=x_batch, x_finite=theta_vec_batch, simulation_positions=sim_ts/T,ctx_simulation_positions=sim_ts/T)\n",
    "        avg_loss += loss.item()*theta_func_batch.shape[0]/len(dataloader.dataset)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}, total loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a948685",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edee7b0",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "fnope_func_samples = torch.zeros(num_samples,10,seq_len).to(device)\n",
    "fnope_vec_samples = torch.zeros(num_samples,10,2).to(device)\n",
    "\n",
    "for true_idx in range(10):\n",
    "    print(f\"Sampling for observation no. {true_idx}\")\n",
    "    #when the model is defined with always_match_x_theta=False and always_equispaced=False, you need to pass the simulation positions for both\n",
    "    #theta and x (simulation_positions and ctx_simulation_positions), EVEN if they're the same. This for BOTH training AND evaluation.\n",
    "    func_samples,vec_samples = model.sample(num_samples, x_o[true_idx].view(-1,seq_len), point_positions = sim_ts/T, ctx_point_positions=sim_ts/T, atol=1e-2, rtol=1e-2)\n",
    "    fnope_func_samples[:,true_idx,:] = func_samples.view(num_samples,seq_len)\n",
    "    fnope_vec_samples[:,true_idx,:] = vec_samples.view(num_samples,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83575b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posterior predictive\n",
    "\n",
    "true_idx = 2\n",
    "\n",
    "fnope_predictive_samples = SIR(fnope_func_samples[:,true_idx,:].view(-1,seq_len),sim_ts, gamma=fnope_vec_samples[:,true_idx,0].view(-1), delta=fnope_vec_samples[:,true_idx,1].view(-1),  likelihood_scale=likelihood_scale, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02973921",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "np_times = sim_ts.cpu().numpy()\n",
    "# 1. Scatter plot of fnope_vec_samples with ground truth (pure matplotlib)\n",
    "axs[0].scatter(\n",
    "    fnope_vec_samples[:, true_idx, 0].cpu().numpy(),\n",
    "    fnope_vec_samples[:, true_idx, 1].cpu().numpy(),\n",
    "    alpha=0.3,\n",
    "    label='Samples'\n",
    ")\n",
    "axs[0].scatter(\n",
    "    theta_vec_o[true_idx, 0].cpu().numpy(),\n",
    "    theta_vec_o[true_idx, 1].cpu().numpy(),\n",
    "    color='red',\n",
    "    label='Ground Truth'\n",
    ")\n",
    "axs[0].set_xlabel('gamma')\n",
    "axs[0].set_ylabel('delta')\n",
    "axs[0].set_title('Posterior samples (vec) with ground truth')\n",
    "axs[0].set_xlim(0, 0.5)\n",
    "axs[0].set_ylim(0, 0.5)\n",
    "axs[0].legend()\n",
    "\n",
    "# 2. Distribution of fnope_func_samples (show mean and 95% interval)\n",
    "mean_func = fnope_func_samples[:, true_idx, :].mean(dim=0).cpu().numpy()\n",
    "lower = fnope_func_samples[:, true_idx, :].quantile(0.025, dim=0).cpu().numpy()\n",
    "upper = fnope_func_samples[:, true_idx, :].quantile(0.975, dim=0).cpu().numpy()\n",
    "axs[1].plot(np_times,mean_func, label='Posterior Mean')\n",
    "axs[1].fill_between(np_times, lower, upper, alpha=0.3, label='95% CI')\n",
    "axs[1].plot(np_times,theta_func_o[true_idx, 0, :].cpu().numpy(), color='red', label='Ground Truth')\n",
    "axs[1].set_title('Posterior func samples')\n",
    "axs[1].legend()\n",
    "\n",
    "# 3. Distribution of fnope_predictive_samples (show mean and 95% interval for first channel)\n",
    "mean_pred = fnope_predictive_samples[:, 0, :].mean(dim=0).cpu().numpy()\n",
    "lower_pred = fnope_predictive_samples[:, 0, :].quantile(0.025, dim=0).cpu().numpy()\n",
    "upper_pred = fnope_predictive_samples[:, 0, :].quantile(0.975, dim=0).cpu().numpy()\n",
    "axs[2].plot(np_times,mean_pred, label='Posterior Predictive Mean')\n",
    "axs[2].fill_between(np_times, lower_pred, upper_pred, alpha=0.3, label='95% CI')\n",
    "axs[2].plot(np_times,x_o[true_idx, 0, :].cpu().numpy(), color='red', label='Ground Truth')\n",
    "axs[2].set_title('Posterior predictive samples')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8bafd",
   "metadata": {},
   "source": [
    "## Evaluate on nonuniform discretizatoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some artificial simulations on nonuniform discretizations\n",
    "from torch.distributions import MultivariateNormal\n",
    "from fnope.simulators.gp_priors import squared_exponential_kernel\n",
    "\n",
    "uneven_seq_len = 100\n",
    "uneven_ts = torch.rand(uneven_seq_len).to(device)\n",
    "uneven_ts = torch.sort(uneven_ts)[0]\n",
    "uneven_ts *= T\n",
    "\n",
    "mean = torch.full((uneven_seq_len,), 0.0).to(device).to(torch.float32)\n",
    "cov = (\n",
    "    squared_exponential_kernel(uneven_ts, uneven_ts, 7.0, 2.5)\n",
    "    + torch.eye(uneven_seq_len).to(device) * 1e-5\n",
    ")\n",
    "mvn = MultivariateNormal(mean, covariance_matrix=cov)\n",
    "\n",
    "\n",
    "\n",
    "theta_func_1 = mvn.sample((10,)).to(device)\n",
    "theta_func_1 = torch.sigmoid(theta_func_1)\n",
    "theta_vec_1 = vec_prior.sample((10,)).to(device)\n",
    "x_1 = SIR(theta_func_1, uneven_ts, gamma=theta_vec_1[:, 0], delta=theta_vec_1[:, 1], likelihood_scale=likelihood_scale, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "fnope_func_samples_1 = torch.zeros(num_samples,10,uneven_seq_len).to(device)\n",
    "fnope_vec_samples_1 = torch.zeros(num_samples,10,2).to(device)\n",
    "\n",
    "for true_idx in range(10):\n",
    "    print(f\"Sampling for observation no. {true_idx}\")\n",
    "    func_samples,vec_samples = model.sample(num_samples, x_1[true_idx].view(-1,uneven_seq_len), point_positions = uneven_ts/T, ctx_point_positions=uneven_ts/T, atol=1e-2, rtol=1e-2)\n",
    "    fnope_func_samples_1[:,true_idx,:] = func_samples.view(num_samples,uneven_seq_len)\n",
    "    fnope_vec_samples_1[:,true_idx,:] = vec_samples.view(num_samples,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6488c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posterior predictive\n",
    "\n",
    "true_idx = 2\n",
    "fnope_predictive_samples = SIR(fnope_func_samples_1[:,true_idx,:].view(-1,uneven_seq_len),uneven_ts, gamma=fnope_vec_samples_1[:,true_idx,0].view(-1), delta=fnope_vec_samples_1[:,true_idx,1].view(-1),  likelihood_scale=likelihood_scale, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068adc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "np_times = uneven_ts.cpu().numpy()\n",
    "\n",
    "# 1. Scatter plot of fnope_vec_samples with ground truth (pure matplotlib)\n",
    "axs[0].scatter(\n",
    "    fnope_vec_samples_1[:, true_idx, 0].cpu().numpy(),\n",
    "    fnope_vec_samples_1[:, true_idx, 1].cpu().numpy(),\n",
    "    alpha=0.3,\n",
    "    label='Samples'\n",
    ")\n",
    "axs[0].scatter(\n",
    "    theta_vec_1[true_idx, 0].cpu().numpy(),\n",
    "    theta_vec_1[true_idx, 1].cpu().numpy(),\n",
    "    color='red',\n",
    "    label='Ground Truth'\n",
    ")\n",
    "axs[0].set_xlabel('gamma')\n",
    "axs[0].set_ylabel('delta')\n",
    "axs[0].set_title('Posterior samples (vec) with ground truth')\n",
    "axs[0].set_xlim(0, 0.5)\n",
    "axs[0].set_ylim(0, 0.5)\n",
    "axs[0].legend()\n",
    "\n",
    "# 2. Distribution of fnope_func_samples (show mean and 95% interval)\n",
    "mean_func = fnope_func_samples_1[:, true_idx, :].mean(dim=0).cpu().numpy()\n",
    "lower = fnope_func_samples_1[:, true_idx, :].quantile(0.025, dim=0).cpu().numpy()\n",
    "upper = fnope_func_samples_1[:, true_idx, :].quantile(0.975, dim=0).cpu().numpy()\n",
    "axs[1].plot(np_times,mean_func, label='Posterior Mean')\n",
    "axs[1].fill_between(np_times, lower, upper, alpha=0.3, label='95% CI')\n",
    "axs[1].plot(np_times,theta_func_1[true_idx, :].cpu().numpy(), color='red', label='Ground Truth')\n",
    "axs[1].set_title('Posterior func samples')\n",
    "axs[1].legend()\n",
    "\n",
    "# 3. Distribution of fnope_predictive_samples (show mean and 95% interval for first channel)\n",
    "mean_pred = fnope_predictive_samples[:, 0, :].mean(dim=0).cpu().numpy()\n",
    "lower_pred = fnope_predictive_samples[:, 0, :].quantile(0.025, dim=0).cpu().numpy()\n",
    "upper_pred = fnope_predictive_samples[:, 0, :].quantile(0.975, dim=0).cpu().numpy()\n",
    "axs[2].plot(np_times,mean_pred, label='Posterior Predictive Mean')\n",
    "axs[2].fill_between(np_times, lower_pred, upper_pred, alpha=0.3, label='95% CI')\n",
    "axs[2].plot(np_times,x_1[true_idx, 0, :].cpu().numpy(), color='red', label='Ground Truth')\n",
    "axs[2].set_title('Posterior predictive samples')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4650d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
